}
# Coeficiente de determinacion R2
Ypred = predict(model, data = datos)
VT = sum( (Yreal-mean(Yreal))*(Yreal-mean(Yreal)) )
VE = sum( (Ypred-mean(Yreal))*(Ypred-mean(Yreal)) )
VR = sum(  (Yreal-Ypred) * (Yreal-mean(Yreal)))
R2 = VE / VT
n = length(Yreal)
p = length((model))
R2.corregido = 1 - (1-R2)*(n-1)/(n-p)
c <- R2
ct <- ifelse(c>0.8,"Si", "No")
d <- format(d,digits = 3)
c <- format(c,digits = 3)
# 1. Test de Normalidad ################ COMPLETAR ###############################
# En 'et' se asigna un "Si" si satisface el test o "No" en otro caso
e <-  residuals(model)
et <-  ifelse(shapiro.test(e)$p.value > .05, "Si", "No")
# 2. Homocedasticidad
library(lmtest)
f <-bptest(model)$p.value
ft <- ifelse(f>=0.05,"Si", "No")
names(ft)<-c()
# 3. Incorrelacion ######################## COMPLETAR ##############################
# En 'gt' se asigna un "Si" si satisface el test o "No" en otro caso
library(lmtest)
gt <- ifelse(dwtest(model,alternative = "two.sided")$p.value > .05, "Si", "No")
# 4. MSE
h <- MSE(datos,model)
h[1] <- format(h[1],digits = 3)
#h[2] <- format(h[2],digits = 3)
# Validacion cruzada
library(bootstrap)
i <- ValidacionCruzada(datos, 10, model)
i[1] <- format(i[1],digits = 3)
i[2] <- format(i[2],digits = 3)
data.frame(EER = at, ANOVA = bt, R2 = ct, Norm = et, Homoced = ft, Incorr = gt, MSE = h[1], CV = i[1], PError = i[2])
}
########################################### EJERCICIOS ######################################################
# Resolucion del ejercicio 1
datos <-data.frame( y=iris$Sepal.Width,
x1=iris$Sepal.Length,
x2=iris$Petal.Length,
x3=iris$Petal.Width)
model.ejercicio1 <- lm(y~., data = datos)
model.ejercicio1.Resultado <- Analisis(datos,model.ejercicio1)
model.ejercicio1.Resultado
install.packages('bootstrap')
library(ISLR)
# Declaracion de funciones
# Calculo de la medida del error standar medio (MSE) y
# el porcentaje de error sobre el rango
# La variable "y" esta en datos[,1]
MSE <- function(datos,regresion){
yprime <- predict(regresion, datos)
b <-sum((datos[,1]-yprime)^2)/length(yprime) ##MSE
b <- as.vector(b)
b[2]<- (b[1]/(range(datos[,1])[2]-range(datos[,1])[1]^2))*100
return(b)
}
# Funciones para realizar la validacion cruzada
library(bootstrap)
theta.fit <- function(v,w,ff=model){
a <- 0
if (ff$call[1]=="lm()"){
a <-lm(as.formula(ff$call$formula), data=as.data.frame(cbind(v,y=w)))
}
else{ if  (ff$call[1]=="gam()"){
a <-gam(as.formula(ff$call$formula), data=as.data.frame(cbind(v,y=w)))
}
}
a
}
theta.predict <- function(fit, x) {
if (is.null(dim(x))) {
x <- t(x)
}
predict(fit, newdata=as.data.frame(x))
}
ValidacionCruzada <- function(datos, particiones, model){
a <- crossval(datos[,-1], datos[,1], theta.fit, theta.predict, ngroup=particiones, ff=model)
b <- (sum(abs(a$cv.fit - datos$y)^2)/length(datos$y)) ##MSE
#Porcentaje de error sobre el rango
c <- sum(abs(a$cv.fit - datos$y))/length(datos$y)
b[2] <- (c*100)/abs(range(datos$y)[2]-range(datos$y)[1])
b <- as.vector(b)
names(b) <- cbind("MSE","%onRange")
return (b)
}
# Visualizacion del ajuste
visualiza_datos <- function(datos, model){
datos_ord <-datos[sort(datos[,1], index.return=TRUE)$ix,]
plot(1:dim(datos_ord)[1],datos_ord$y,xlab="ejemplo", ylab="y",type="p")
pred <- predict(model, newdata = datos_ord)
points(1:dim(datos_ord)[1],pred, col="red")
plot(1:dim(datos_ord)[1],datos_ord$y,xlab="ejemplo", ylab="y",type="l")
segments(1:dim(datos_ord)[1], datos_ord$y, 1:dim(datos_ord)[1], pred,col="red", lty = 1)
}
AnalisisGrafico <- function (datos, model){
par(mfrow=c(2,2))
# histograma Normalidad
e <-residuals(model)
d <- e/summary(model)$sigma
hist (d, probability = T, xlab = "Errores estandar", main = "", xlim = c(-3,3))
d.seq <- seq(-3,3,length = 50)
lines(d.seq, dnorm(d.seq, mean(d), sd(d)), col="red")
# Incorrelacion
n <- length(d)
plot(d[1:n-1],d[2:n],xlab = "Error i", ylab = "Error i-1")
lines(lowess(d[1:n-1],d[2:n]),col="red")
# Representacion del resultado
visualiza_datos(datos,model)
}
# Procesamiento de propiedades del modelo
Analisis <- function (datos, model){
resumen_model = summary(model)
# Error Estandar Residual
a <-  100 * (resumen_model$sigma/(mean(datos$y)))
#a <-100*(sd(datos$y)/(mean(datos$y)))
at <- ifelse(a<10,"Si","No")
#a <- format(a,digits = 3)
# ANOVA
b <- 0
bt <- "--"
if (model$call[1]!="gam()"){
b <- pf(resumen_model$fstatistic[1L],
resumen_model$fstatistic[2L],
resumen_model$fstatistic[3L],
lower.tail = FALSE)
bt <- ifelse(b<0.05,"Si", "No")
names(bt) <- c()
b <- format(b,digits = 3)
}
# Coeficiente de determinacion R2
Ypred = predict(model, data = datos)
VT = sum( (Yreal-mean(Yreal))*(Yreal-mean(Yreal)) )
VE = sum( (Ypred-mean(Yreal))*(Ypred-mean(Yreal)) )
VR = sum(  (Yreal-Ypred) * (Yreal-mean(Yreal)))
R2 = VE / VT
n = length(Yreal)
p = length((model))
R2.corregido = 1 - (1-R2)*(n-1)/(n-p)
c <- R2
ct <- ifelse(c>0.8,"Si", "No")
d <- format(d,digits = 3)
c <- format(c,digits = 3)
# 1. Test de Normalidad ################ COMPLETAR ###############################
# En 'et' se asigna un "Si" si satisface el test o "No" en otro caso
e <-  residuals(model)
et <-  ifelse(shapiro.test(e)$p.value > .05, "Si", "No")
# 2. Homocedasticidad
library(lmtest)
f <-bptest(model)$p.value
ft <- ifelse(f>=0.05,"Si", "No")
names(ft)<-c()
# 3. Incorrelacion ######################## COMPLETAR ##############################
# En 'gt' se asigna un "Si" si satisface el test o "No" en otro caso
library(lmtest)
gt <- ifelse(dwtest(model,alternative = "two.sided")$p.value > .05, "Si", "No")
# 4. MSE
h <- MSE(datos,model)
h[1] <- format(h[1],digits = 3)
#h[2] <- format(h[2],digits = 3)
# Validacion cruzada
library(bootstrap)
i <- ValidacionCruzada(datos, 10, model)
i[1] <- format(i[1],digits = 3)
i[2] <- format(i[2],digits = 3)
data.frame(EER = at, ANOVA = bt, R2 = ct, Norm = et, Homoced = ft, Incorr = gt, MSE = h[1], CV = i[1], PError = i[2])
}
########################################### EJERCICIOS ######################################################
# Resolucion del ejercicio 1
datos <-data.frame( y=iris$Sepal.Width,
x1=iris$Sepal.Length,
x2=iris$Petal.Length,
x3=iris$Petal.Width)
model.ejercicio1 <- lm(y~., data = datos)
model.ejercicio1.Resultado <- Analisis(datos,model.ejercicio1)
model.ejercicio1.Resultado
datos <-data.frame( y=iris$Petal.Length,
x1=iris$Sepal.Length,
x2=iris$Sepal.Width,
x3=iris$Petal.Width)
model.ejercicio2 <- lm(y~., data = datos)
model.ejercicio2.Resultado <- Analisis(datos,model.ejercicio2)
model.ejercicio2.Resultado
# Comparacion entre los modelos del ejercicio 1 y el ejercicio2
df <- data.frame(rbind(ejer1=model.ejercicio1.Resultado,
ejer2=model.ejercicio2.Resultado),
stringsAsFactors = FALSE)
df
datos <-data.frame(  y=trees$Girth,
x1=trees$Height,
x2=trees$Volume)
datos
datos <-data.frame(  y=trees$Girth,
x1=trees$Height,
x2=trees$Volume)
model.ejercicio3 <- lm(y~., data = datos)
model.ejercicio3.Resultado <- Analisis(datos,model.ejercicio3)
model.ejercicio3.Resultado
AnalisisGrafico (datos, model.ejercicio3)
df <- data.frame(rbind(ejer1=model.ejercicio1.Resultado,
ejer2=model.ejercicio2.Resultado,
ejer3=model.ejercicio3.Resultado),
stringsAsFactors = FALSE)
df
datos <-data.frame( y=iris$Sepal.Width,
x1=iris$Sepal.Length,
x2=iris$Petal.Length,
x3=iris$Petal.Width)
########## Completar desde aqui
model1<-lm(y~poly(x1,3),data=datos)
model2<-lm(y~poly(x2,3),data=datos)
model3<-lm(y~poly(x3,3),data=datos)
model4<-lm(y~poly(x1,3)+poly(x2,3)+poly(x3,3),data=datos)
model5<-lm(y~poly(x1,3)*poly(x2,3)*poly(x3,3),data=datos)
model.ejercicio41.Resultado=Analisis(datos,model1)
model.ejercicio42.Resultado=Analisis(datos,model2)
model.ejercicio43.Resultado=Analisis(datos,model3)
model.ejercicio44.Resultado=Analisis(datos,model4)
model.ejercicio45.Resultado=Analisis(datos,model5)
df <- data.frame(rbind(model1=model.ejercicio41.Resultado,
model2=model.ejercicio42.Resultado,
model3=model.ejercicio43.Resultado,
model4=model.ejercicio44.Resultado,
model5=model.ejercicio45.Resultado),
stringsAsFactors = FALSE)
df
# Gr?ficos
install.packages("ggplot2")
# Gr?ficos
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
library(devtools)
install_github("ggbiplot", "vqv")
install_github("ggbiplot", "vqv/ggbiplot")
install_github("ggbiplot", "vqv")
install_github("ggbiplot", "vqv/ggbiplot")
install_github("ggbiplot", "vqv/ggbiplot/ggbiplot")
install_github("ggbiplot", "vqv/ggbiplot/ggbiplot/ggbiplot")
install.packages("rgl")     #plot3D
install.packages("rgl",dependencies = T)     #plot3D
install.packages("rgl",dependencies = T)     #plot3D
install.packages("rgl",dependencies = T)     #plot3D
install.packages("rgl",dependencies = T)     #plot3D
librry("rgl")
library("rgl")
library("GGally")
# 1-variate
install.packages("outliers")  # Grubb
install.packages("EnvStats")  # Rosner
# Multi-variate -Mahalanobis-
install.packages("mvoutlier")  #MCD ChiC
install.packages("CerioliOutlierDetection")  #MCD Hardin Rocke estimaci?n robusta de la matriz de covarianzas
install.packages("robustbase")
install.packages("mvnormtest")   # Test Normalidad multivariante
install.packages("MASS")         # Para cov.rob estimaci?n robusta de la matriz de covarianzas
# Multivariate Unsupervised
install.packages("DMwR")  #lof
install.packages("cluster")
install.packages("rjava")
install.packages("RKEEL")
install.packages("rJava")
install.packages("rJava")
install.packages("RKEEL")
install.packages("RWeka")
Acierto <- function(y1,y2){
return (sum (sapply(1:length(y1), function(x){
if (is.na(y2[x])){
0
}
else if (as.numeric(y1[x])==as.numeric(y2[x])){
1
}
else{
0
}
}))/length(y1))
}
library(ISLR)
library(splines)
bd <- read.csv("CoordenadasMunicipios.csv", header = T, sep = ",")
setwd("/home/cris/mrcrstnherediagmez@gmail.com/Preprocesamiento_Y_clasificacion")
bd <- read.csv("CoordenadasMunicipios.csv", header = T, sep = ",")
bd[,1] = as.numeric(bd[,1])
datos = as.data.frame(cbind(y= bd[,1], x1 = bd$longitud, x2 = bd$latitud))
set.seed(9)
muestra = sample(1:nrow(datos), 100)
train = as.data.frame(datos[-muestra, ])
test = as.data.frame(datos[muestra, ])
# modelo regresion logistica
d1 = as.data.frame(cbind(y= as.numeric(I(train$y==1)), x1 =train$x1, x2 =train$x2))
d2 = as.data.frame(cbind(y= as.numeric(I(train$y==2)), x1 =train$x1, x2 =train$x2))
d3 = as.data.frame(cbind(y= as.numeric(I(train$y==3)), x1 =train$x1, x2 =train$x2))
d4 = as.data.frame(cbind(y= as.numeric(I(train$y==4)), x1 =train$x1, x2 =train$x2))
d5 = as.data.frame(cbind(y= as.numeric(I(train$y==5)), x1 =train$x1, x2 =train$x2))
d6 = as.data.frame(cbind(y= as.numeric(I(train$y==6)), x1 =train$x1, x2 =train$x2))
d7 = as.data.frame(cbind(y= as.numeric(I(train$y==7)), x1 =train$x1, x2 =train$x2))
d8 = as.data.frame(cbind(y= as.numeric(I(train$y==8)), x1 =train$x1, x2 =train$x2))
mRL1 <- glm(y ~ ns(x1,4) + ns(x2,4) , data = d1)
mRL2 <- glm(y ~ ns(x1,4) + ns(x2,4) , data = d2)
mRL3 <- glm(y ~ ns(x1,4) + ns(x2,4) , data = d3)
mRL4 <- glm(y ~ ns(x1,4) + ns(x2,4) , data = d4)
mRL5 <- glm(y ~ ns(x1,4) + ns(x2,4) , data = d5)
mRL6 <- glm(y ~ ns(x1,4) + ns(x2,4) , data = d6)
mRL7 <- glm(y ~ ns(x1,4) + ns(x2,4) , data = d7)
mRL8 <- glm(y ~ ns(x1,4) + ns(x2,4) , data = d8)
SmRL <- cbind (predict(mRL1, newdata = train, type="response"),
predict(mRL2, newdata = train, type="response"),
predict(mRL3, newdata = train, type="response"),
predict(mRL4, newdata = train, type="response"),
predict(mRL5, newdata = train, type="response"),
predict(mRL6, newdata = train, type="response"),
predict(mRL7, newdata = train, type="response"),
predict(mRL8, newdata = train, type="response"))
salida = sapply(1:nrow(SmRL), function(x) {which.max(SmRL[x,])})
Acierto(train$y,salida)
SmRLTest <- cbind (predict(mRL1, newdata = test, type="response"),
predict(mRL2, newdata = test, type="response"),
predict(mRL3, newdata = test, type="response"),
predict(mRL4, newdata = test, type="response"),
predict(mRL5, newdata = test, type="response"),
predict(mRL6, newdata = test, type="response"),
predict(mRL7, newdata = test, type="response"),
predict(mRL8, newdata = test, type="response"))
salidaTest = sapply(1:nrow(SmRLTest), function(x) {which.max(SmRLTest[x,])})
Acierto(test$y,salidaTest)
plot(train$x1,train$x2,col=train$y)
plot(train$x1,train$x2,col=train$y)
points(train$x1, train$x2, col=train$y, pch = "x")
# Pintado de espacios de decision
grid.lines = 200
x.pred <- seq(min(datos$x2), max(datos$x2), length.out = grid.lines)
z.pred <- seq(min(datos$x1), max(datos$x1), length.out = grid.lines)
xz <- expand.grid( x2 = x.pred, x1 = z.pred)
SmRLTotal <- cbind (predict(mRL1, newdata = xz, type="response"),
predict(mRL2, newdata = xz, type="response"),
predict(mRL3, newdata = xz, type="response"),
predict(mRL4, newdata = xz, type="response"),
predict(mRL5, newdata = xz, type="response"),
predict(mRL6, newdata = xz, type="response"),
predict(mRL7, newdata = xz, type="response"),
predict(mRL8, newdata = xz, type="response"))
salidaTotal1 = sapply(1:nrow(SmRLTotal), function(x) {which.max(SmRLTotal[x,])})
plot(xz[,2], xz[,1], col= salidaTotal1, pch='*')
points(datos$x1, datos$x2, col=datos$y, pch = "x")
library(tree)
train[,1] = as.factor(train[,1])
mT <- tree(y ~ x1 + x2 , data = train)
mT
tree
library(tree)
train[,1] = as.factor(train[,1])
mT <- tree(y ~ x1 + x2 , data = train)
train
grid.lines = 200
tree$x1
grid.lines = 200
x.pred <- seq(min(train$x2), max(train$x2), length.out = grid.lines)
z.pred <- seq(min(train$x1), max(train$x1), length.out = grid.lines)
xz <- expand.grid( x2 = x.pred, x1 = z.pred)
train$y
train
library(tree)
train[,1] = as.factor(train[,1])
mT <- tree(y ~ x1 + x2 , data = train)
train
train$y
train
predict(mT, newdata = xz, type="response")
xz
plot(xz[,2], xz[,1], col= salidaTotal1, pch='*')
train[,1] = as.factor(train[,1])
mT <- tree(y ~ x1 + x2 , data = train)
smT<-predict(mT, newdata = train, type="class")
smTest<-predict(mT, newdata = test, type="class")
Acierto(train$y,smT)
Acierto(test$y,smTest)
grid.lines = 200
x.pred <- seq(min(datos$x2), max(datos$x2), length.out = grid.lines)
z.pred <- seq(min(datos$x1), max(datos$x1), length.out = grid.lines)
xz <- expand.grid( x2 = x.pred, x1 = z.pred)
SmRLTotal <- predict(mT, newdata = xz, type="class")
salidaTotal1 = sapply(1:nrow(SmRLTotal), function(x) {which.max(SmRLTotal[x,])})
plot(xz[,2], xz[,1], col= salidaTotal1, pch='*')
train[,1] = as.factor(train[,1])
mT <- tree(y ~ x1 + x2 , data = train)
smT<-predict(mT, newdata = train, type="class")
smTest<-predict(mT, newdata = test, type="class")
Acierto(train$y,smT)
Acierto(test$y,smTest)
grid.lines = 200
x.pred <- seq(min(datos$x2), max(datos$x2), length.out = grid.lines)
z.pred <- seq(min(datos$x1), max(datos$x1), length.out = grid.lines)
xz <- expand.grid( x2 = x.pred, x1 = z.pred)
SmRLTotal <- predict(mT, newdata = xz, type="class")
plot(xz[,2], xz[,1], col= SmRLTotal, pch='*')
points(train$x1, train$x2, col=train$y, pch = "x")
?points
points(train$x2, train$x1, col=train$y, pch = "x")
plot(xz[,2], xz[,1], col= SmRLTotal, pch='*')
points(train$x2, train$x1, col=train$y, pch = "x")
points(train$x2, train$x1, col=train$y, pch = "x")
train[,1] = as.factor(train[,1])
mT <- tree(y ~ x1 + x2 , data = train)
smT<-predict(mT, newdata = train, type="class")
smTest<-predict(mT, newdata = test, type="class")
Acierto(train$y,smT)
Acierto(test$y,smTest)
grid.lines = 200
x.pred <- seq(min(datos$x2), max(datos$x2), length.out = grid.lines)
z.pred <- seq(min(datos$x1), max(datos$x1), length.out = grid.lines)
xz <- expand.grid( x2 = x.pred, x1 = z.pred)
SmRLTotal <- predict(mT, newdata = xz, type="class")
plot(xz[,2], xz[,1], col= SmRLTotal, pch='*')
points(train$x2, train$x1, col=train$y, pch = "x")
points(train$x1, train$x2, col=train$y, pch = "x")
plot(xz[,2], xz[,1], col= SmRLTotal, pch='*')
points(train$x1, train$x2, col=train$y, pch = "x")
plot(xz[,2], xz[,1], col= SmRLTotal, pch='*')
plot(xz[,2], xz[,1], col= SmRLTotal, pch='*')
points(train$x2, train$x1, col=train$y, pch = "x")
points(train$x1, train$x2, col=train$y, pch = "x")
# Random Forest
library (randomForest)
set.seed (1)
bag = randomForest(y ~ x1 + x2, data=train)
bag
set.seed (1)
bag = randomForest(y ~ x1 + x2, data=train)
bag
yhat.bag.train = predict (bag ,newdata =train)
yhat.bag.test= predict (bag ,newdata =test)
Acierto(train$y,yhat.bag.train)
Acierto(test$y,yhat.bag.test)
grid.lines = 200
x.pred <- seq(min(datos$x2), max(datos$x2), length.out = grid.lines)
z.pred <- seq(min(datos$x1), max(datos$x1), length.out = grid.lines)
xz <- expand.grid( x2 = x.pred, x1 = z.pred)
SmRLTotal <- predict(bag, newdata = xz, type="class")
plot(xz[,2], xz[,1], col= SmRLTotal, pch='*')
points(train$x1, train$x2, col=train$y, pch = "x")
SmRforest <- predict(bag, newdata = xz, type="class")
plot(xz[,2], xz[,1], col= SmRforest, pch='*')
points(train$x1, train$x2, col=train$y, pch = "x")
set.seed (1)
boost.datos =gbm(y ~ x1 + x2, data=train,
distribution="multinomial",n.trees =5000,
interaction.depth =4)
library (gbm)
set.seed (1)
boost.datos =gbm(y ~ x1 + x2, data=train,
distribution="multinomial",n.trees =5000,
interaction.depth =4)
# con multinomial indicamos que lo queremos para clasificación
summary(boost.datos)
yhat.boost.train=predict (boost.datos ,newdata =train,
n.trees =5000)
yhat.boost.test=predict (boost.datos ,newdata =test,
n.trees =5000)
Acierto(train$y,yhat.boost.train)
Acierto(test$y,yhat.boost.test)
boost.datos =gbm(y ~ x1 + x2, data=train,
distribution="multinomial",n.trees =500,
interaction.depth =10)
# con multinomial indicamos que lo queremos para clasificación
summary(boost.datos)
yhat.boost.train=predict (boost.datos ,newdata =train,
n.trees =5000)
yhat.boost.test=predict (boost.datos ,newdata =test,
n.trees =5000)
Acierto(train$y,yhat.boost.train)
Acierto(test$y,yhat.boost.test)
boost.datos =gbm(y ~ x1 + x2, data=train,
distribution="multinomial",n.trees =500,
interaction.depth =10)
# con multinomial indicamos que lo queremos para clasificación
summary(boost.datos)
yhat.boost.train=predict (boost.datos ,newdata =train,n.trees =5000)
yhat.boost.test=predict (boost.datos ,newdata =test, n.trees =5000)
yhat.boost.train=predict (boost.datos ,newdata =train)
yhat.boost.train=predict (boost.datos ,newdata =train,n.trees =500)
Acierto(train$y,yhat.boost.train)
Acierto(test$y,yhat.boost.test)
set.seed (2)
boost.datos =gbm(y ~ x1 + x2, data=train,
distribution="multinomial",n.trees =500,
interaction.depth =10)
# con multinomial indicamos que lo queremos para clasificación
summary(boost.datos)
yhat.boost.train=predict (boost.datos ,newdata =train,n.trees =500)
yhat.boost.test=predict (boost.datos ,newdata =test, n.trees =5000)
Acierto(train$y,yhat.boost.train)
Acierto(test$y,yhat.boost.test)
set.seed (1)
boost.datos =gbm(y ~ x1 + x2, data=train,
distribution="multinomial",n.trees =500,
interaction.depth =10)
# con multinomial indicamos que lo queremos para clasificación
summary(boost.datos)
yhat.boost.train=predict (boost.datos ,newdata =train,n.trees =500)
yhat.boost.test=predict (boost.datos ,newdata =test, n.trees =500)
yhat.boost.test = matrix(yhat.boost.test,ncol=(length(yhat.boost.test)/nrow(test)))
yhat.boost.test
yhat.boost.y = sapply(1:nrow(test), function(x){
which.max(yhat.boost.test[x,])
})
(sum(yhat.boost.y == as.numeric(test[,5]))/nrow(test))*100
(sum(yhat.boost.y == as.numeric(test[,1]))/nrow(test))*100
(sum(yhat.boost.y == as.numeric(test[,2]))/nrow(test))*100
